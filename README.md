# ğŸ” RAG Embedding Comparison: Sentence-Transformers vs OpenAI

This repository documents a practical experiment in **Retrieval-Augmented Generation (RAG)** aimed at understanding how different embedding models behave in real-world, numeric-heavy customer support scenarios.

---

## ğŸ¯ Objective

The primary goal of this project was to investigate **how well different embedding models handle numeric identifiers (e.g., account numbers, broadband IDs, service IDs) in retrieval-based search.**

Specifically, we wanted to answer:

> *If a user asks a question containing a numeric identifier, can the retrieval system reliably find the correct record?*

Example test question:

> **â€œWhich agent handled the customer with broadband 80020000008?â€**

---

## ğŸ› ï¸ What We Built

To study this, we built a **dual-index RAG system** with:

### ğŸ”¹ Data
- 100 synthetic customer support call center events  
- Each event represents a customer interaction with:
  - Event ID  
  - Event type  
  - Intent and sub-intent  
  - Timestamp  
  - Customer service transcript  
  - Numeric identifiers (e.g., broadband IDs, account numbers, service IDs)

Each **event/transcript is treated as a single semantic chunk**.

---

### ğŸ”¹ Two Parallel Retrieval Pipelines (FAISS)

We indexed the same dataset using two different embedding approaches:

| Index | Embedding Model | Vector Store |
|------|----------------|-------------|
| **Index 1** | `sentence-transformers/all-MiniLM-L6-v2` | FAISS |
| **Index 2** | `text-embedding-3-small` (OpenAI) | FAISS |

This allowed us to compare retrieval behavior **side-by-side on identical data**.

---

## ğŸ“± Streamlit App (Interactive Comparison)

The Streamlit application allows users to:

- Enter **one question**
- Retrieve results from **both indexes simultaneously**
- Compare:
  - Retrieved chunks  
  - LLM-generated answers grounded in each retrieval  

### Home Screen (Question Input)
<img src="images/app_home.png" width="100%">

### Retrieval Results (Side-by-Side Comparison)
<img src="images/app_results.png" width="100%">

---

## ğŸ” Key Observation (Core Finding)

### ğŸ§ª Experiment Result

When querying with a **numeric identifier**, for example:

> *â€œWhich agent handled customer with broadband **80020000008**?â€*

### âœ… OpenAI Embeddings
- Successfully retrieved the correct transcript  
- Identified the correct agent  
- Generated an accurate, grounded answer  

### âŒ Sentence-Transformers
- Struggled to retrieve the correct record  
- Often retrieved semantically similar but numerically incorrect events  
- Resulted in incorrect or incomplete answers  

---

## ğŸ¤” What This Suggests (Hypothesis)

While we did not conclusively prove the root cause, our observations suggest:

- **Sentence-Transformers embeddings may not preserve numeric specificity as effectively as OpenAI embeddings.**
- This could be due to:
  - Tokenization differences  
  - How numbers are represented in embedding space  
  - Training data biases  
  - Loss of precision for long numeric sequences  

This is an important consideration for RAG systems that rely heavily on **IDs, account numbers, transaction numbers, or service identifiers.**

---

## ğŸ“Œ Why This Matters (Practical Implications)

For enterprise RAG systems dealing with:
- Customer support tickets  
- Banking records  
- Telecom accounts  
- Order management systems  
- Healthcare IDs  

ğŸ‘‰ Choosing the **right embedding model can significantly impact retrieval accuracy**, especially when numeric identifiers are involved.

---

## ğŸš€ Live Demo

ğŸ‘‰ **https://rag-embedding-comparison-mhnzfby6qxsghmeb92zkhc.streamlit.app/**

Try queries like:

- â€œWhich agent handled customer with broadband 80020000008?â€
- â€œShow me the transcript for service ID 90030000005â€
- "What happens when sim replacement request is placed by user?"
